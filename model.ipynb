{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1,1,1,1,1/2,1/4,1/8,1/16, 1/32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module): # single convolutional layer\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None #remove bias of conv layer\n",
    "\n",
    "        #initialize conv weights and bias\n",
    "        nn.init.normal_(self.conv.weight)  # init weights to standard guasiann\n",
    "        nn.init.zeros_(self.bias) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale)  + self.bias.view(1, self.bias.shape[0], 1,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module): # normalize across RGB channels\n",
    "    def __init__(self):\n",
    "        super().__init__() #call init of parent class nn.Module (super class)\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward ( self, x ):   # x is the input tensor to the forward function\n",
    "        return x / torch.sqrt(torch.mean(x**2,  dim = 1, keepdim = True) + self.epsilon) #dim = 1 means we want to take the mean across the channels, keepdim = True means we want to keep the dimension of the mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module): #convolutional block 3x3 conv, pixel norm, leaky relu\n",
    "    def __init__(self, in_channels, out_channels, use_pn = True):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)   #conv layer 1\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pn = use_pn\n",
    "\n",
    "    def forward (self, x ):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x #if use_pn is true, then we apply pixel norm\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        return self.pn(x) if self.use_pn else x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()   #list of modules, list of rgb layers\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range( len( factors ) - 1, 0 , 1 ): \n",
    "            conv_in_c = int( in_channels * factors[i] )# multiply in_channels by the factor to get the number of channels for the next block\n",
    "            conv_out_c = int( in_channels * factors[ i - 1 ])  # multiply in_channels by the factor to get the number of channels for the next block\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c, use_pn=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels, conv_in_c, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        self.inital_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1, padding=0)  \n",
    "        self.rgb_layers.append(self.inital_rgb) # add the initial rgb layer to the rgb layers list\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)   # average pooling layer\n",
    "\n",
    "        self.final_block = nn.Sequential(\n",
    "            # +1 to in_channels because we concatenate from MiniBatch std\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),  # we use this instead of linear layer\n",
    "        )\n",
    "\n",
    "\n",
    "    def fade_in(self, alpha ,downscaled_apool, out_conv):\n",
    "        return alpha * out_conv + (1 - alpha) * downscaled_apool\n",
    "    \n",
    "    def minibatch_std(self, x):\n",
    "        batch_stats = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        return torch.cat([x, batch_stats], dim=1) # we want to add a new channel to the input tensor that contains the standard deviation of each feature map across the batch\n",
    "\n",
    "    def forward(self, x, alpha,  steps):\n",
    "        # steps * 4 but need last index\n",
    "        cur_step = len( self.prog_blocks ) - steps \n",
    "        out = self.leaky(self.rgb_layers[cur_step](x)) # pass x through the rgb layer of the current step and apply leaky relu to the output \n",
    "\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "        \n",
    "        downscaled = self.leaky( self.rgb_layers[cur_step + 1] ( self.avg_pool(x) ) ) # pass x through the rgb layer of the next step by downscaling x by a factor of 2 using average pooling and apply leaky relu to the output\n",
    "        out = self.prog_blocks[cur_step](out) # pass the output of the rgb layer through the conv block of the current step\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range( cur_step + 1, len ( self.prog_blocks) ): # inital layer done, as we go up the prog factors, make smaller to 4x4\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1) # pass the output of the minibatch std layer through the final block and flatten the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, kernel_size=4, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm()\n",
    "        )\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1, padding=0) #1x1 conv layer\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([self.initial_rgb]) #list of modules, list of rgb layers\n",
    "        for i in range(len(factors) - 1 ) : # -1 because we dont want to include the last layer\n",
    "            conv_in_c = int( in_channels * factors[i]) # multiply in_channels by the factor to get the number of channels for the next block\n",
    "            conv_out_c = int( in_channels * factors[i+1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c)) # conv block is conv layer, pixel norm, leaky relu\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)) # we want to add a 1x1 conv layer to the rgb layers list\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        \"\"\"\n",
    "        alpha: fade in factor   \n",
    "        upscaled: output of the previous block\n",
    "        generated: output of the current block\n",
    "        \"\"\"\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward (self,x, alpha, steps): # steps * 4 \n",
    "        out = self.initial(x) # pass x through the initial block (4x4)\n",
    "\n",
    "        if steps == 0: \n",
    "            return self.initial_rgb(out) # if steps = 0, then we return the output of the initial rgb layer\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscale = F.interpolate(out, scale_factor=2, mode=\"nearest\") # upscale the output of the previous block\n",
    "            out = self.prog_blocks[step](upscale) # pass the upscaled output through the conv block\n",
    "        \n",
    "        final_upscale = self.rgb_layers[steps - 1](upscale)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscale, final_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 4, steps: 0\n",
      "torch.Size([1, 3, 4, 4])\n",
      "Success! At img size: 4\n",
      "Image size: 8, steps: 1\n",
      "torch.Size([1, 3, 8, 8])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amant\\Projects\\DL\\ProGan\\model.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(z\u001b[39m.\u001b[39mshape )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39massert\u001b[39;00m z\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, img_size, img_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out \u001b[39m=\u001b[39m critic(z, alpha\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, steps\u001b[39m=\u001b[39;49mnum_steps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39massert\u001b[39;00m out\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSuccess! At img size: \u001b[39m\u001b[39m{\u001b[39;00mimg_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\amant\\Projects\\DL\\ProGan\\model.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_block(out)\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m downscaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleaky( \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrgb_layers[cur_step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] ( \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavg_pool(x) ) ) \u001b[39m# pass x through the rgb layer of the next step by downscaling x by a factor of 2 using average pooling and apply leaky relu to the output\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprog_blocks[cur_step](out) \u001b[39m# pass the output of the rgb layer through the conv block of the current step\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfade_in(alpha, downscaled, out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#X10sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m( cur_step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m ( \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprog_blocks) ): \u001b[39m# inital layer done, as we go up the prog factors, make smaller to 4x4\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:282\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mvalues())[idx])\n\u001b[0;32m    281\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_abs_string_index(idx)]\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:272\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    270\u001b[0m idx \u001b[39m=\u001b[39m operator\u001b[39m.\u001b[39mindex(idx)\n\u001b[0;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m idx \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)):\n\u001b[1;32m--> 272\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is out of range\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(idx))\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m idx \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    274\u001b[0m     idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of range"
     ]
    }
   ],
   "source": [
    "# train code\n",
    "if __name__ == \"__main__\":\n",
    "    Z_DIM = 100\n",
    "    IN_CHANNELS = 256\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "    critic = Discriminator(IN_CHANNELS, img_channels=3)\n",
    "\n",
    "    for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "        num_steps = int(np.log2(img_size / 4))\n",
    "        print(f\"Image size: {img_size}, steps: {num_steps}\")\n",
    "        x = torch.randn((1, Z_DIM, 1, 1))\n",
    "        z = gen(x, 0.5, steps=num_steps)\n",
    "        print(z.shape )\n",
    "        assert z.shape == (1, 3, img_size, img_size)\n",
    "        out = critic(z, alpha=0.5, steps=num_steps)\n",
    "        assert out.shape == (1, 1)\n",
    "        print(f\"Success! At img size: {img_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
