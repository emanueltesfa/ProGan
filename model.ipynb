{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\amant\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\lib\\nvfuser_codegen.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\amant\\Projects\\DL\\ProGan\\model.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amant/Projects/DL/ProGan/model.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\__init__.py:122\u001b[0m\n\u001b[0;32m    120\u001b[0m     err \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mWinError(last_error)\n\u001b[0;32m    121\u001b[0m     err\u001b[39m.\u001b[39mstrerror \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Error loading \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdll\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m or one of its dependencies.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    123\u001b[0m \u001b[39melif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     is_loaded \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\amant\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\lib\\nvfuser_codegen.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "factors = [1,1,1,1,1/2,1/4,1/8,1/16, 1/32]\n",
    "\n",
    "\n",
    "class WSConv2d(nn.Module): # single convolutional layer\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None #remove bias of conv layer\n",
    "\n",
    "        #initialize conv weights and bias\n",
    "        nn.init.normal_(self.conv.weight)  # init weights to standard guasiann\n",
    "        nn.init.zeros_(self.bias) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale)  + self.bias.view(1, self.bias.shape[0], 1,1)\n",
    "    \n",
    "\n",
    "\n",
    "class PixelNorm(nn.Module): # normalize across RGB channels\n",
    "    def __init__(self):\n",
    "        super().__init__() #call init of parent class nn.Module (super class)\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "        def forward ( self, x ):   # x is the input tensor to the forward function\n",
    "            return x / torch.sqrt(torch.mean(x**2,  dim = 1, keepdim = True) + self.epsilon) #dim = 1 means we want to take the mean across the channels, keepdim = True means we want to keep the dimension of the mean\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module): #convolutional block 3x3 conv, pixel norm, leaky relu\n",
    "    def __init__(self, in_channels, out_channels, use_pn = True):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)   #conv layer 1\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pn = use_pn\n",
    "\n",
    "        def forward (self, x ):\n",
    "            x = self.leaky(self.conv1(x))\n",
    "            x = self.pn(x) if self.use_pn else x #if use_pn is true, then we apply pixel norm\n",
    "            x = self.leaky(self.conv2(x))\n",
    "            return self.pn(x) if self.use_pn else x\n",
    "            \n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super.__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, kernel_size=4, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(z_dim, in_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm()\n",
    "        )\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1) #1x1 conv layer\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList(self.inital_rgb) #list of modules, list of rgb layers\n",
    "        for i in range(len(factors) -1 ): # -1 because we dont want to include the last layer\n",
    "            conv_in_c = int( in_channels * factors[i]) # multiply in_channels by the factor to get the number of channels for the next block\n",
    "            conv_out_c = int( in_channels * factors[i+1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c)) # conv block is conv layer, pixel norm, leaky relu\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1)) # we want to add a 1x1 conv layer to the rgb layers list\n",
    "\n",
    "        def fade_in(self, alpha, upscaled, generated):\n",
    "            \"\"\"\n",
    "            alpha: fade in factor   \n",
    "            upscaled: output of the previous block\n",
    "            generated: output of the current block\n",
    "            \"\"\"\n",
    "            return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "        def forward (self,x, alpha, steps): # steps * 4 \n",
    "            out = self.initial(x) # pass x through the initial block (4x4)\n",
    "\n",
    "            if steps == 0: \n",
    "                return self.initial_rgb(out) # if steps = 0, then we return the output of the initial rgb layer\n",
    "            \n",
    "            for step in range(steps):\n",
    "                upscale = F.interpolate(out, scale_factor=2, mode=\"nearest\") # upscale the output of the previous block\n",
    "                out = self.prog_blocks[step](upscale) # pass the upscaled output through the conv block\n",
    "            \n",
    "            final_upscale = self.rgb_layers[steps - 1](upscale)\n",
    "            final_out = self.rgb_layers[steps](out)\n",
    "\n",
    "            return self.fade_in(alpha, final_upscale, final_out)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super.__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()   #list of modules, list of rgb layers\n",
    "        self.leaky = nn.LeaklyReLu(0.2)\n",
    "\n",
    "        for i in range( len( factors - 1 ), 0 , 1 ): \n",
    "            conv_in_c = int( in_channels * factors[i]) # multiply in_channels by the factor to get the number of channels for the next block\n",
    "            conv_out_c = int( in_channels * factors[i -1 ] ) # multiply in_channels by the factor to get the number of channels for the next block\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c, use_pn=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels, conv_in_c, kernel_size=1, stride=1))\n",
    "        \n",
    "        self.inital_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1)  \n",
    "        self.rgb_layers.append(self.initial_rgb) # add the initial rgb layer to the rgb layers list\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)   # average pooling layer\n",
    "\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels + 1 , in_channels=2, kernel_size=3, stride=1, padding=1), # 3x3 Conv,  513 in channels = in channels + 1 \n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels=2, out_channels=1, kernel_size=4, stride=1, padding=0), # 4x4 Conv \n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, 1, kernel_size=1, stride = 1 , padding=0) # final 1x1 conv layer\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha ,downscaled_apool, out_conv):\n",
    "        return alpha * out_conv + (1 - alpha) * downscaled_apool\n",
    "    \n",
    "    def minibatch_std(self, x):\n",
    "        batch_stats = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        return torch.cat([x, batch_stats], dim=1) # we want to add a new channel to the input tensor that contains the standard deviation of each feature map across the batch\n",
    "\n",
    "    def forward(self, x, alpha,  steps):\n",
    "        # steps * 4 but need last index\n",
    "        cur_step = len( self.prog_blocks ) - steps \n",
    "        out = self.leaky(self.rgb_layers[cur_step](x)) # pass x through the rgb layer of the current step and apply leaky relu to the output \n",
    "\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "        \n",
    "        downscaled = self.leaky( self.rgb_layers[cur_step + 1] ( self.avg_pool(x) ) ) # pass x through the rgb layer of the next step by downscaling x by a factor of 2 using average pooling and apply leaky relu to the output\n",
    "        out = self.prog_blocks(cur_step)(out) # pass the output of the rgb layer through the conv block of the current step\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range( cur_step + 1, len ( self.prog_blocks) ): # inital layer done, as we go up the prog factors, make smaller to 4x4\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1) # pass the output of the minibatch std layer through the final block and flatten the output\n",
    "\n",
    "# train code\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Z_DIM = 128\n",
    "    IN_CHANNELS = 256\n",
    "    generator = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "    discriminator = Discriminator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "    \n",
    "    for img_size in [4, 8, 16, 32, 64, 128, 256, 512]:\n",
    "        num_steps = int(np.log2(img_size / 4))\n",
    "        x = torch.randn((1, Z_DIM, 1, 1))\n",
    "        z = generator(x, alpha=1.0, steps=num_steps)\n",
    "        assert z.shape == (1, 3, img_size, img_size)\n",
    "        out = discriminator(z, alpha=1.0, steps=num_steps)\n",
    "        assert out.shape == (1, 1)\n",
    "        print(f\"Success! At img size {img_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
